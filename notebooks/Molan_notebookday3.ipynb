{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3caf92e-1f22-4f53-8487-361310bf115a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    GlobalMaxPooling1D,\n",
    "    Concatenate,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---- Paths & labels ----\n",
    "DATA_DIR = \"/home/jupyter/old_backup\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train_sent_emo.csv\")\n",
    "DEV_CSV = os.path.join(DATA_DIR, \"dev_sent_emo.csv\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test_sent_emo.csv\")\n",
    "\n",
    "CLASSES = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "label2id = {c: i for i, c in enumerate(CLASSES)}\n",
    "id2label = {i: c for c, i in label2id.items()}\n",
    "\n",
    "\n",
    "# ---- Enhanced data loading ----\n",
    "def load_meld_data():\n",
    "    \"\"\"Load and preprocess MELD dataset with enhanced preprocessing\"\"\"\n",
    "    use_cols = [\"Utterance\", \"Emotion\"]\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_CSV)[use_cols].dropna()\n",
    "        dev_df = pd.read_csv(DEV_CSV)[use_cols].dropna()\n",
    "        test_df = pd.read_csv(TEST_CSV)[use_cols].dropna()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Map emotions to labels and clean up\n",
    "    for name, df in zip((\"train\", \"dev\", \"test\"), (train_df, dev_df, test_df)):\n",
    "        df[\"label\"] = df[\"Emotion\"].map(label2id)\n",
    "        df.dropna(subset=[\"label\"], inplace=True)\n",
    "        df.rename(columns={\"Utterance\": \"text\"}, inplace=True)\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    print(f\"Loaded - Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Check label distribution\n",
    "    print(\"\\nLabel distribution in training data:\")\n",
    "    label_counts = train_df[\"Emotion\"].value_counts()\n",
    "    for emotion, count in label_counts.items():\n",
    "        print(f\"  {emotion}: {count}\")\n",
    "\n",
    "    return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "# ---- Enhanced text preprocessing ----\n",
    "def enhanced_clean_text(text):\n",
    "    \"\"\"Enhanced text cleaning with emotion-preserving features\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Preserve important punctuation patterns for emotions\n",
    "    text = re.sub(r\"!{2,}\", \" multiexclaim \", text)  # Multiple exclamations\n",
    "    text = re.sub(r\"\\?{2,}\", \" multiquestion \", text)  # Multiple questions\n",
    "    text = re.sub(r\"\\.{3,}\", \" ellipsis \", text)  # Ellipsis\n",
    "\n",
    "    # Handle repeated characters (e.g., \"sooo\" -> \"so repeatchar\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1 repeatchar\", text)\n",
    "\n",
    "    # Preserve emoticons and basic punctuation\n",
    "    text = re.sub(r\"[^\\w\\s!?.,:\\-\\(\\)]\", \" \", text)\n",
    "\n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_df, dev_df, test_df = load_meld_data()\n",
    "if train_df is None:\n",
    "    print(\"Data loading failed. Please check your file paths.\")\n",
    "    exit()\n",
    "\n",
    "# Apply enhanced preprocessing\n",
    "print(\"Applying enhanced text preprocessing...\")\n",
    "train_df[\"text_clean\"] = train_df[\"text\"].apply(enhanced_clean_text)\n",
    "dev_df[\"text_clean\"] = dev_df[\"text\"].apply(enhanced_clean_text)\n",
    "test_df[\"text_clean\"] = test_df[\"text\"].apply(enhanced_clean_text)\n",
    "\n",
    "# Remove empty texts\n",
    "train_df = train_df[train_df[\"text_clean\"].str.len() > 0]\n",
    "dev_df = dev_df[dev_df[\"text_clean\"].str.len() > 0]\n",
    "test_df = test_df[test_df[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "print(\n",
    "    f\"After cleaning - Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\"\n",
    ")\n",
    "\n",
    "# ---- Enhanced tokenization ----\n",
    "vocab_size = 15000  # Increased vocabulary size\n",
    "max_len = 150  # Increased sequence length\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=vocab_size,\n",
    "    oov_token=\"<OOV>\",\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    filters=\"\",  # Don't remove punctuation we want to keep\n",
    ")\n",
    "\n",
    "print(\"Building enhanced vocabulary...\")\n",
    "tokenizer.fit_on_texts(train_df[\"text_clean\"])\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text_clean\"])\n",
    "X_dev = tokenizer.texts_to_sequences(dev_df[\"text_clean\"])\n",
    "X_test = tokenizer.texts_to_sequences(test_df[\"text_clean\"])\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_dev_pad = pad_sequences(X_dev, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_dev = dev_df[\"label\"].values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\n",
    "    f\"Final shapes - Train: {X_train_pad.shape}, Dev: {X_dev_pad.shape}, Test: {X_test_pad.shape}\"\n",
    ")\n",
    "\n",
    "# ---- Calculate class weights for imbalanced data ----\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "\n",
    "# ---- Enhanced Model Architecture ----\n",
    "def create_advanced_emotion_model(\n",
    "    vocab_size, embedding_dim=200, max_len=150, num_classes=7\n",
    "):\n",
    "    \"\"\"Create an advanced emotion classification model with multiple improvements\"\"\"\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "\n",
    "    # Embedding layer with larger dimensions\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len,\n",
    "        mask_zero=True,\n",
    "        embeddings_regularizer=l2(0.0001),\n",
    "    )(input_layer)\n",
    "\n",
    "    # First Bidirectional LSTM layer\n",
    "    lstm1 = Bidirectional(\n",
    "        LSTM(\n",
    "            128,\n",
    "            return_sequences=True,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            kernel_regularizer=l2(0.0001),\n",
    "        )\n",
    "    )(embedding)\n",
    "    lstm1 = BatchNormalization()(lstm1)\n",
    "\n",
    "    # Second Bidirectional LSTM layer\n",
    "    lstm2 = Bidirectional(\n",
    "        LSTM(\n",
    "            64,\n",
    "            return_sequences=True,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            kernel_regularizer=l2(0.0001),\n",
    "        )\n",
    "    )(lstm1)\n",
    "    lstm2 = BatchNormalization()(lstm2)\n",
    "\n",
    "    # Global Max Pooling to capture most important features\n",
    "    global_max_pool = GlobalMaxPooling1D()(lstm2)\n",
    "\n",
    "    # Alternative: use the last output from LSTM\n",
    "    lstm3 = Bidirectional(\n",
    "        LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)\n",
    "    )(lstm2)\n",
    "\n",
    "    # Concatenate different representations\n",
    "    concat_features = Concatenate()([global_max_pool, lstm3])\n",
    "\n",
    "    # Dense layers with batch normalization\n",
    "    dense1 = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.001))(\n",
    "        concat_features\n",
    "    )\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "\n",
    "    dense2 = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.001))(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Dropout(0.4)(dense2)\n",
    "\n",
    "    dense3 = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.001))(dense2)\n",
    "    dense3 = Dropout(0.3)(dense3)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation=\"softmax\")(dense3)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the enhanced model\n",
    "actual_vocab_size = min(vocab_size, len(tokenizer.word_index) + 1)\n",
    "model = create_advanced_emotion_model(\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=200,\n",
    "    max_len=max_len,\n",
    "    num_classes=len(CLASSES),\n",
    ")\n",
    "\n",
    "# Enhanced optimizer with learning rate scheduling\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ---- Enhanced training callbacks ----\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.001,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.3,\n",
    "        patience=4,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1,\n",
    "        min_delta=0.001,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ---- Train enhanced model ----\n",
    "print(\"Starting enhanced training...\")\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=16,  # Smaller batch size for better convergence\n",
    "    epochs=30,  # More epochs with early stopping\n",
    "    validation_data=(X_dev_pad, y_dev),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict,  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# ---- Enhanced evaluation ----\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predictions with confidence scores\n",
    "y_pred_proba = model.predict(X_test_pad, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"Average prediction confidence: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "\n",
    "# ---- Enhanced Classification Report ----\n",
    "def enhanced_classification_report(y_true, y_pred, class_names):\n",
    "    \"\"\"Enhanced classification report with additional metrics\"\"\"\n",
    "\n",
    "    report = {}\n",
    "    total_samples = len(y_true)\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Basic metrics\n",
    "        tp = np.sum((y_true == i) & (y_pred == i))\n",
    "        fp = np.sum((y_true != i) & (y_pred == i))\n",
    "        fn = np.sum((y_true == i) & (y_pred != i))\n",
    "        tn = np.sum((y_true != i) & (y_pred != i))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        support = np.sum(y_true == i)\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        report[class_name] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1,\n",
    "            \"support\": support,\n",
    "            \"specificity\": specificity,\n",
    "        }\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate enhanced classification report\n",
    "report = enhanced_classification_report(y_test, y_pred_classes, CLASSES)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENHANCED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8} {'Specificity':<12}\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for class_name, metrics in report.items():\n",
    "    print(\n",
    "        f\"{class_name:<12} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} \"\n",
    "        f\"{metrics['f1-score']:<10.4f} {metrics['support']:<8} {metrics['specificity']:<12.4f}\"\n",
    "    )\n",
    "\n",
    "# Calculate macro and weighted averages\n",
    "macro_f1 = np.mean([metrics[\"f1-score\"] for metrics in report.values()])\n",
    "weighted_f1 = np.sum(\n",
    "    [metrics[\"f1-score\"] * metrics[\"support\"] for metrics in report.values()]\n",
    ") / len(y_test)\n",
    "\n",
    "print(f\"\\nMacro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n",
    "print(f\"Overall Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# ---- Error Analysis ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find most confused classes\n",
    "confusion_pairs = {}\n",
    "for true_idx, pred_idx in zip(y_test, y_pred_classes):\n",
    "    if true_idx != pred_idx:\n",
    "        pair = (id2label[true_idx], id2label[pred_idx])\n",
    "        confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_confusion = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Most frequent confusions:\")\n",
    "for (true_label, pred_label), count in sorted_confusion[:10]:\n",
    "    print(f\"  {true_label} -> {pred_label}: {count} times\")\n",
    "\n",
    "# ---- Save model (optional) ----\n",
    "# model.save('enhanced_emotion_model.h5')\n",
    "# print(\"Model saved as 'enhanced_emotion_model.h5'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2de5b-0cc4-456c-9340-9d8e1da0e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
