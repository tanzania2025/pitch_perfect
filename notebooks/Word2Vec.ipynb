{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e960deac-8e7e-4312-8c47-f459adeeda83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m venv fresh_env\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e3325c-85f2-4c37-8702-21eb9a63a4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # In Jupyter/Colab: use %pip so it installs into the current kernel\n",
    "# %pip uninstall -y tensorflow keras protobuf\n",
    "# %pip install -U \"tensorflow==2.12.1\" \"keras==2.12.0\" \"protobuf==3.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a4d861-b599-4af5-880a-5f65333c2208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow==2.12.1\" \"keras==2.12.0\" \"protobuf==3.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892a59a9-67d6-402c-9bb2-2621b917b9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 15:12:09.096404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 2: After kernel restart, run this main code =====\n",
    "\n",
    "##############################################\n",
    "# === MELD Emotion Classification (Dependency-Free) ===\n",
    "##############################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# ---- Paths & labels ----\n",
    "# Point to the correct directory\n",
    "DATA_DIR = \"/home/jupyter/old_backup\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train_sent_emo.csv\")\n",
    "DEV_CSV = os.path.join(DATA_DIR, \"dev_sent_emo.csv\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test_sent_emo.csv\")\n",
    "\n",
    "CLASSES = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "label2id = {c: i for i, c in enumerate(CLASSES)}\n",
    "id2label = {i: c for c, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fe5a67-8d0a-4e30-8ff3-c61504d503e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ---- Load data ----\n",
    "def load_meld_data():\n",
    "    \"\"\"Load and preprocess MELD dataset\"\"\"\n",
    "    use_cols = [\"Utterance\", \"Emotion\"]\n",
    "\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_CSV)[use_cols].dropna()\n",
    "        dev_df = pd.read_csv(DEV_CSV)[use_cols].dropna()\n",
    "        test_df = pd.read_csv(TEST_CSV)[use_cols].dropna()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure the MELD CSV files are in the correct directory\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Map emotions to labels\n",
    "\n",
    "    for name, df in zip((\"train\", \"dev\", \"test\"), (train_df, dev_df, test_df)):\n",
    "        df[\"label\"] = df[\"Emotion\"].map(label2id)\n",
    "        df.dropna(subset=[\"label\"], inplace=True)\n",
    "        df.rename(columns={\"Utterance\": \"text\"}, inplace=True)\n",
    "\n",
    "    print(f\"Loaded - Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Check label distribution\n",
    "    print(\"\\nLabel distribution in training data:\")\n",
    "    label_counts = train_df[\"Emotion\"].value_counts()\n",
    "    for emotion, count in label_counts.items():\n",
    "        print(f\"  {emotion}: {count}\")\n",
    "\n",
    "    return train_df, dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd0566f-06f6-4c30-b335-097f3c924878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded - Train: 9989, Dev: 1109, Test: 2610\n",
      "\n",
      "Label distribution in training data:\n",
      "  neutral: 4710\n",
      "  joy: 1743\n",
      "  surprise: 1205\n",
      "  anger: 1109\n",
      "  sadness: 683\n",
      "  disgust: 271\n",
      "  fear: 268\n"
     ]
    }
   ],
   "source": [
    "train_df, dev_df, test_df = load_meld_data()\n",
    "\n",
    "if train_df is None:\n",
    "    print(\"Data loading failed. Please check your file paths.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# ---- Text preprocessing ----\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()\n",
    "    # Keep basic punctuation that might be emotionally relevant\n",
    "    text = re.sub(r\"[^\\w\\s!?.,]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df[\"text_clean\"] = train_df[\"text\"].apply(clean_text)\n",
    "dev_df[\"text_clean\"] = dev_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text_clean\"] = test_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b280b53b-55f1-4b42-8cb8-e0c9d2243438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.16.2 keras==3.3.3 protobuf==4.25.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2b9025-ccec-4484-bd30-e3e8107155fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning - Train: 9989, Dev: 1109, Test: 2610\n",
      "Building vocabulary...\n",
      "Vocabulary size: 5364\n",
      "Most common words: [('<OOV>', 1), ('i', 2), ('you', 3), ('the', 4), ('s', 5), ('it', 6), ('to', 7), ('a', 8), ('that', 9), ('and', 10)]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df[\"text_clean\"].str.len() > 0]\n",
    "dev_df = dev_df[dev_df[\"text_clean\"].str.len() > 0]\n",
    "test_df = test_df[test_df[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "print(\n",
    "    f\"After cleaning - Train: {len(train_df)}, Dev: {len(dev_df)}, Test: {len(test_df)}\"\n",
    ")\n",
    "\n",
    "# ---- Tokenization ----\n",
    "# Create vocabulary from training data\n",
    "vocab_size = 10000\n",
    "max_len = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", lower=True, split=\" \")\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "tokenizer.fit_on_texts(train_df[\"text_clean\"])\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Most common words: {list(tokenizer.word_index.items())[:10]}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text_clean\"])\n",
    "X_dev = tokenizer.texts_to_sequences(dev_df[\"text_clean\"])\n",
    "X_test = tokenizer.texts_to_sequences(test_df[\"text_clean\"])\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_dev_pad = pad_sequences(X_dev, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fcfa0a-1328-4340-8f62-daa00e411a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes - Train: (9989, 128), Dev: (1109, 128), Test: (2610, 128)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 128, 128)          686720    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 833,223\n",
      "Trainable params: 833,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get labels\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_dev = dev_df[\"label\"].values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\n",
    "    f\"Final shapes - Train: {X_train_pad.shape}, Dev: {X_dev_pad.shape}, Test: {X_test_pad.shape}\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Model Architecture ----\n",
    "def create_emotion_model(vocab_size, embedding_dim=128, max_len=128, num_classes=7):\n",
    "    \"\"\"Create the emotion classification model\"\"\"\n",
    "    model = Sequential(\n",
    "        [\n",
    "            # Embedding layer\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=max_len,\n",
    "                mask_zero=True,\n",
    "            ),\n",
    "            # Bidirectional LSTM layers\n",
    "            Bidirectional(\n",
    "                LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
    "            ),\n",
    "            Bidirectional(\n",
    "                LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)\n",
    "            ),\n",
    "            # Dense layers\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    # Dense(128, activation='relu'),\n",
    "    # Dropout(0.5),\n",
    "    # Dense(64, activation='relu'),\n",
    "    # Dropout(0.4),\n",
    "    # Dense(32, activation='relu'),\n",
    "    # Dropout(0.3),\n",
    "    # Dense(num_classes, activation='softmax')])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "actual_vocab_size = min(vocab_size, len(tokenizer.word_index) + 1)\n",
    "model = create_emotion_model(\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    max_len=max_len,\n",
    "    num_classes=len(CLASSES),\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a511ae43-2861-4627-8207-bd635876d4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---- Training callbacks ----\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-7, verbose=1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950176e2-b580-4e1e-b6d4-e055121c43d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/20\n",
      "313/313 [==============================] - 323s 945ms/step - loss: 1.5886 - accuracy: 0.4720 - val_loss: 1.5523 - val_accuracy: 0.4617 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 281s 899ms/step - loss: 1.4278 - accuracy: 0.5087 - val_loss: 1.5178 - val_accuracy: 0.4572 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 289s 923ms/step - loss: 1.3189 - accuracy: 0.5246 - val_loss: 1.5537 - val_accuracy: 0.4626 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 313s 999ms/step - loss: 1.2062 - accuracy: 0.5676 - val_loss: 1.5813 - val_accuracy: 0.4436 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.1045 - accuracy: 0.6082\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "313/313 [==============================] - 278s 886ms/step - loss: 1.1045 - accuracy: 0.6082 - val_loss: 1.7317 - val_accuracy: 0.4436 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 284s 906ms/step - loss: 0.9814 - accuracy: 0.6542 - val_loss: 1.7877 - val_accuracy: 0.4418 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 271s 867ms/step - loss: 0.9369 - accuracy: 0.6737 - val_loss: 1.9425 - val_accuracy: 0.4319 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.8845 - accuracy: 0.6964Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "313/313 [==============================] - 287s 917ms/step - loss: 0.8845 - accuracy: 0.6964 - val_loss: 2.0303 - val_accuracy: 0.4292 - lr: 5.0000e-04\n",
      "Epoch 8: early stopping\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Accuracy: 0.4870\n"
     ]
    }
   ],
   "source": [
    "# ---- Train model ----\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_dev_pad, y_dev),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ---- Evaluation ----\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69a1a0-1d8c-4f37-b843-702b1825f0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test_pad, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# ---- Classification Report (Manual Implementation) ----\n",
    "def classification_report_manual(y_true, y_pred, class_names):\n",
    "    \"\"\"Manual classification report to avoid sklearn dependency\"\"\"\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    report = {}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # True positives, false positives, false negatives\n",
    "        tp = np.sum((y_true == i) & (y_pred == i))\n",
    "        fp = np.sum((y_true != i) & (y_pred == i))\n",
    "        fn = np.sum((y_true == i) & (y_pred != i))\n",
    "\n",
    "        # Calculate precision, recall, f1\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        support = np.sum(y_true == i)\n",
    "\n",
    "        report[class_name] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1,\n",
    "            \"support\": support,\n",
    "        }\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate manual classification report\n",
    "report = classification_report_manual(y_test, y_pred_classes, CLASSES)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for class_name, metrics in report.items():\n",
    "    print(\n",
    "        f\"{class_name:<12} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} \"\n",
    "        f\"{metrics['f1-score']:<10.4f} {metrics['support']:<8}\"\n",
    "    )\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = np.mean(y_test == y_pred_classes)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba78e3-0494-4a9c-8b47-3915dd263e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---- Plot training history (if matplotlib available) ----\n",
    "def plot_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Accuracy plot\n",
    "        ax1.plot(history.history[\"accuracy\"], label=\"Training\")\n",
    "        ax1.plot(history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "        ax1.set_title(\"Model Accuracy\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Loss plot\n",
    "        ax2.plot(history.history[\"loss\"], label=\"Training\")\n",
    "        ax2.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "        ax2.set_title(\"Model Loss\")\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available - skipping plots\")\n",
    "\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "print(\n",
    "    f\"\\nTraining completed! Best validation accuracy: {max(history.history['val_accuracy']):.4f}\"\n",
    ")\n",
    "\n",
    "# ---- Sample predictions ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a few test examples\n",
    "sample_indices = np.random.choice(len(test_df), min(5, len(test_df)), replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    original_idx = test_df.index[idx]\n",
    "    text = test_df.loc[original_idx, \"text\"]\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred_classes[idx]\n",
    "    confidence = np.max(y_pred[idx])\n",
    "\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\n",
    "        f\"True: {CLASSES[true_label]} | Predicted: {CLASSES[pred_label]} | Confidence: {confidence:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1ae09-7d92-4861-8514-63247a55987c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
